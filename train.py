# -*- coding: utf-8 -*-
"""Pegasus+PGN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iJx2nrxxd4pT6Z3AL6N69069Vk1BaeDl
"""

# export HF_DATASETS_CACHE="./dataset_cache"

import os
import sys
import json
import time
import torch
import pickle
import random
import numpy as np

import utils
from utils import set_random_seed, format_time
import transformers
from tqdm import tqdm
from pathlib import Path
from datasets import load_from_disk
from Model import PointerPegasus
from transformers import PegasusConfig, PegasusTokenizer
from transformers import get_linear_schedule_with_warmup
from torch.optim import AdamW
from torch.nn import CrossEntropyLoss
from torch.utils.data import Dataset, DataLoader
from torch.utils.data.sampler import RandomSampler, SequentialSampler

seed = 42
utils.set_random_seed(seed)

save_dir = Path(os.getenv("SAVE_DIR", None))
tmp_dir = Path('./tmp/tmpmodel.pt')
original_checkpoint = 'google/pegasus-cnn_dailymail'
device = 'cuda' if torch.cuda.is_available() else 'cpu'
config = PegasusConfig.from_pretrained(original_checkpoint)
tokenizer = PegasusTokenizer.from_pretrained(original_checkpoint)

print("-------------------Loading Data-------------------")
dataset_dir = Path("./dataset_cache")
tokenized_training = load_from_disk(dataset_dir)
tokenized_validation = load_from_disk(dataset_dir)
tokenized_test = load_from_disk(dataset_dir)

batch_size = 1
train_loader = DataLoader(tokenized_training,
                          sampler=RandomSampler(tokenized_training),
                          num_workers=2,
                          batch_size=batch_size)
val_loader = DataLoader(tokenized_validation,
                        sampler=SequentialSampler(tokenized_validation),
                        num_workers=2,
                        batch_size=batch_size)
test_loader = DataLoader(tokenized_test,
                         sampler=SequentialSampler(tokenized_test),
                         num_workers=2,
                         batch_size=1)

print("-------------------Data Loaded-------------------")

epochs = 7
learning_rate = 5e-5
warmup_ratio = 0.1
epsilon = 1e-8
batch_size = 64
total_steps = (len(tokenized_training) // batch_size) * epochs if len(tokenized_training) % batch_size == 0 else (len(tokenized_training) // batch_size + 1) * epochs
sample_every = 100

print("-------------------Creating A Instance for the Model-------------------")

model = PointerPegasus(original_checkpoint, tokenizer, device).to(device)

print("-------------------Instance Created-------------------")

optimizer = AdamW(model.Pointer.parameters(),
                  lr=learning_rate,
                  eps=epsilon)
scheduler = get_linear_schedule_with_warmup(optimizer, 
                                            num_warmup_steps=warmup_ratio*total_steps,
                                            num_training_steps=total_steps)
loss_fct = CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)


def validation(model, val_loader):
    # ========================================
    #               Validation
    # ========================================
    print("")
    print("Running Validation...")

    t0 = time.time()

    model.eval()

    total_eval_loss = 0

    # Evaluate data for one epoch
    for step, sample in enumerate(val_loader):

        input_ids = torch.LongTensor(sample["input_ids"]).to(device)
        labels = torch.LongTensor(sample["decoder_input_ids"]).to(device)
        masks = torch.LongTensor(sample["attention_mask"]).to(device)
        with torch.no_grad():
            return_dict = model(input_ids=input_ids,
                                decoder_input_ids=labels,
                                attention_mask=masks)
        logits = return_dict["logits"]

        logits = logits.contiguous()
        labels = labels.contiguous()

        loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))

        total_eval_loss += loss.item()
        if step % 1000 == 0:
            print(step)
        #break

    avg_val_loss = total_eval_loss / len(val_loader)
    validation_time = format_time(time.time() - t0)

    print("-------------------Validation Loss: {0:.5f}-------------------".format(avg_val_loss))
    print("-------------------Validation took: {:}-------------------".format(validation_time))

    return avg_val_loss, validation_time


def train(state, train_loader, val_loader, sample_every=5000, batch_size=8, max_grad_norm=1.0):
    total_t0 = time.time()
    if state is None:
        last_epoch = 0
        step = 0
        tr_loss = 0
        avg_val_loss = -1
        training_time = None
        validation_time = None
        best_val_loss = 100
    else:
        last_epoch = state['epoch']
        step = state['step']
        train_loader = state['train_loader']
        model.load_state_dict(state['model'])
        scheduler.load_state_dict(state['scheduler'])
        optimizer.load_state_dict(state['optimizer'])
        tr_loss = state['Total Training Loss']
        avg_val_loss = state['Validation Loss']
        training_time = state['Training Time']
        validation_time = state['Validation Time']
        best_val_loss = state['Best Validation Loss']

    for epoch_i in range(last_epoch, epochs):

        # ========================================
        #               Training
        # ========================================
        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))
        print('Training...')

        t0 = time.time()

        
        model.zero_grad()
        global_step = 0

        pbar = tqdm(total=len(tokenized_training))
        pbar.update(step)

        for sample in train_loader:
            # train the whole model
            model.train()

            input_ids = torch.LongTensor(sample["input_ids"]).to(device)
            labels = torch.LongTensor(sample["decoder_input_ids"]).to(device)
            masks = torch.LongTensor(sample["attention_mask"]).to(device)

            output = model.forward(input_ids=input_ids,
                                   decoder_input_ids=labels,
                                   attention_mask=masks)
            gen_probs, final_probs = output
            
            final_probs = final_probs.contiguous()
            labels = labels.contiguous()

            loss = loss_fct(final_probs.view(-1, final_probs.size(-1)), labels.view(-1))
            loss = loss/batch_size

            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.Pointer.parameters(), max_grad_norm)
            tr_loss += loss.item()

            step += 1
            pbar.update(1)
            
            if (step % batch_size == 0) or (step == len(tokenized_training)):
                optimizer.step()
                scheduler.step()  # Update learning rate schedule
                optimizer.zero_grad()
                # 1 global_step = 1 batch = 8 steps
                global_step += 1
                # Calculate the average loss over all of the batches.
                avg_train_loss = tr_loss / global_step
                # Measure how long this epoch took.
                training_time = format_time(time.time() - t0)
                
                if global_step % sample_every == 0:
                    model.eval()
                    with torch.no_grad():
                        sample_tgt = model.generate(input_ids=input_ids, max_new_tokens=128)[0]
                        # Generate 1 sample every 'sample_every' global_steps (8*'sample_every' steps)
                        print("{} {}".format(len(sample_tgt), tokenizer.decode(sample_tgt)), end='\n\n')
                        print("lr : {} tr_loss: {}".format(scheduler.get_last_lr()[0], avg_train_loss), end='\n\n')

                        # Evaluate the model every 10000 global steps (80000 steps)
                        if global_step % 10000 ==0:
                            avg_val_loss, validation_time = validation(model, val_loader)
                            print("lr : {} tr_loss: {} val_loss: {}".format(scheduler.get_last_lr()[0], avg_train_loss, avg_val_loss), end='\n\n')

                            if avg_val_loss < best_val_loss:
                                best_val_loss = avg_val_loss
                                torch.save(state, save_dir)
                                tokenizer.save_pretrained(save_dir)
                                print("model saved on step {} with val loss: {}", step, best_val_loss)

                    # save the training status every 'sample_every' global_steps (8*'sample_every' steps)
                    state = {'epoch': epoch_i,
                             'step': step,
                             'model': model.state_dict(),
                             'train_loader': train_loader,
                             'optimizer': optimizer.state_dict(),
                             'scheduler': scheduler.state_dict(),
                             'Total Training Loss': tr_loss,
                             'Validation Loss': avg_val_loss,
                             'Training Time': training_time,
                             'Validation Time': validation_time,
                             'Best Validation Loss': best_val_loss}
                    
                    torch.save(state, tmp_dir)
                    tokenizer.save_pretrained(save_dir)
                        
                break

            # continue training from the last break point
            if step == len(tokenized_training):
                step = 0
                break
        pbar.close()        
        break

        

        print("")
        print("-------------------Average training loss: {0:.2f}-------------------".format(avg_train_loss))
        print("-------------------Training epoch took: {:}-------------------".format(training_time))
            
        
        #break

    print("")
    print("Training complete!")
    print("Total training took {:} (h:mm:ss)".format(format_time(time.time()-total_t0)))


if os.path.exists(tmp_dir):
    print("  Loading training stats file from {}".format(tmp_dir))
    state = torch.load(tmp_dir, map_location=torch.device('cuda'))
else:
    print("No training record found...Start from new training")
    state = None

train(state,
      train_loader=train_loader, 
      val_loader=val_loader, 
      sample_every=500)